{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46eed391",
   "metadata": {},
   "source": [
    "## Linear methods for regression and classification\n",
    "In this jupyter notebook, we will practice the topics covered in the lectures. Specially, we will do hands-on practice of:\n",
    "- load and prepare data for machine learning model training and testing\n",
    "- train and test linear models (linear regression (lasso/ridge), polynomial regression, and logistic regression)\n",
    "- compare and understand model performance \n",
    "\n",
    "For implementations that have fixed results, we provide running examples for your reference. *You might get slightly different results due to the sklearn version you are using, just leave a comment to indicate your version where you get different results.*<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5acb55f",
   "metadata": {},
   "source": [
    "## Linear Regression and Polynomial Regression\n",
    "In this section, we will explore the diabetes dataset:\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_diabetes.html#sklearn.datasets.load_diabetes <br>\n",
    "\n",
    "This dataset contains n = 442 diabetes patients' information of ten variables: age, sex, body mass index, average blood pressure, and six blood serum measurements. Each patient has a quantitative value of disease progression one year after baseline.\n",
    "\n",
    "\n",
    "We will fit different regression models to analyze this dataset: \n",
    "The steps include:\n",
    "1. Basic data exploration:\n",
    "    > what does the data look like (#samples, #features) <br>\n",
    "    > the feature matrix and description of each feature <br>\n",
    "    > the target variable <br>\n",
    "    \n",
    "2. Prepare data for model training and testing <br>\n",
    "\n",
    "3. Fit different regression models (linear/lasso/ridge) on the training set and evaluate model performance on the testing set <br>\n",
    "\n",
    "4. Compare and understand model performance through interpreting coefficients.\n",
    "\n",
    "**Note:** please always add comments to explain your observations/findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff1d6c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eab86d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6253bf61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36fb790a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are different ways to load the dataset, please make sure you understand the mechanism\n",
    "# reference: https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_diabetes.html\n",
    "data = load_diabetes(as_frame=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db355e95",
   "metadata": {},
   "source": [
    "### Basic dataset exploration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94e585c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((442, 10), (442,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.data.shape, data.target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83f34331",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age', 'sex', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "abcc0287",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>bmi</th>\n",
       "      <th>bp</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>s6</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.038076</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.061696</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>-0.044223</td>\n",
       "      <td>-0.034821</td>\n",
       "      <td>-0.043401</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.019907</td>\n",
       "      <td>-0.017646</td>\n",
       "      <td>151.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.001882</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.051474</td>\n",
       "      <td>-0.026328</td>\n",
       "      <td>-0.008449</td>\n",
       "      <td>-0.019163</td>\n",
       "      <td>0.074412</td>\n",
       "      <td>-0.039493</td>\n",
       "      <td>-0.068332</td>\n",
       "      <td>-0.092204</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.085299</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.044451</td>\n",
       "      <td>-0.005670</td>\n",
       "      <td>-0.045599</td>\n",
       "      <td>-0.034194</td>\n",
       "      <td>-0.032356</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.002861</td>\n",
       "      <td>-0.025930</td>\n",
       "      <td>141.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.089063</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.011595</td>\n",
       "      <td>-0.036656</td>\n",
       "      <td>0.012191</td>\n",
       "      <td>0.024991</td>\n",
       "      <td>-0.036038</td>\n",
       "      <td>0.034309</td>\n",
       "      <td>0.022688</td>\n",
       "      <td>-0.009362</td>\n",
       "      <td>206.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.005383</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.036385</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>0.003935</td>\n",
       "      <td>0.015596</td>\n",
       "      <td>0.008142</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>-0.031988</td>\n",
       "      <td>-0.046641</td>\n",
       "      <td>135.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>0.041708</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.019662</td>\n",
       "      <td>0.059744</td>\n",
       "      <td>-0.005697</td>\n",
       "      <td>-0.002566</td>\n",
       "      <td>-0.028674</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.031193</td>\n",
       "      <td>0.007207</td>\n",
       "      <td>178.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>-0.005515</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>-0.015906</td>\n",
       "      <td>-0.067642</td>\n",
       "      <td>0.049341</td>\n",
       "      <td>0.079165</td>\n",
       "      <td>-0.028674</td>\n",
       "      <td>0.034309</td>\n",
       "      <td>-0.018114</td>\n",
       "      <td>0.044485</td>\n",
       "      <td>104.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>0.041708</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>-0.015906</td>\n",
       "      <td>0.017293</td>\n",
       "      <td>-0.037344</td>\n",
       "      <td>-0.013840</td>\n",
       "      <td>-0.024993</td>\n",
       "      <td>-0.011080</td>\n",
       "      <td>-0.046883</td>\n",
       "      <td>0.015491</td>\n",
       "      <td>132.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>-0.045472</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.001215</td>\n",
       "      <td>0.016318</td>\n",
       "      <td>0.015283</td>\n",
       "      <td>-0.028674</td>\n",
       "      <td>0.026560</td>\n",
       "      <td>0.044529</td>\n",
       "      <td>-0.025930</td>\n",
       "      <td>220.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>-0.045472</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.073030</td>\n",
       "      <td>-0.081413</td>\n",
       "      <td>0.083740</td>\n",
       "      <td>0.027809</td>\n",
       "      <td>0.173816</td>\n",
       "      <td>-0.039493</td>\n",
       "      <td>-0.004222</td>\n",
       "      <td>0.003064</td>\n",
       "      <td>57.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>442 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          age       sex       bmi        bp        s1        s2        s3  \\\n",
       "0    0.038076  0.050680  0.061696  0.021872 -0.044223 -0.034821 -0.043401   \n",
       "1   -0.001882 -0.044642 -0.051474 -0.026328 -0.008449 -0.019163  0.074412   \n",
       "2    0.085299  0.050680  0.044451 -0.005670 -0.045599 -0.034194 -0.032356   \n",
       "3   -0.089063 -0.044642 -0.011595 -0.036656  0.012191  0.024991 -0.036038   \n",
       "4    0.005383 -0.044642 -0.036385  0.021872  0.003935  0.015596  0.008142   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "437  0.041708  0.050680  0.019662  0.059744 -0.005697 -0.002566 -0.028674   \n",
       "438 -0.005515  0.050680 -0.015906 -0.067642  0.049341  0.079165 -0.028674   \n",
       "439  0.041708  0.050680 -0.015906  0.017293 -0.037344 -0.013840 -0.024993   \n",
       "440 -0.045472 -0.044642  0.039062  0.001215  0.016318  0.015283 -0.028674   \n",
       "441 -0.045472 -0.044642 -0.073030 -0.081413  0.083740  0.027809  0.173816   \n",
       "\n",
       "           s4        s5        s6  target  \n",
       "0   -0.002592  0.019907 -0.017646   151.0  \n",
       "1   -0.039493 -0.068332 -0.092204    75.0  \n",
       "2   -0.002592  0.002861 -0.025930   141.0  \n",
       "3    0.034309  0.022688 -0.009362   206.0  \n",
       "4   -0.002592 -0.031988 -0.046641   135.0  \n",
       "..        ...       ...       ...     ...  \n",
       "437 -0.002592  0.031193  0.007207   178.0  \n",
       "438  0.034309 -0.018114  0.044485   104.0  \n",
       "439 -0.011080 -0.046883  0.015491   132.0  \n",
       "440  0.026560  0.044529 -0.025930   220.0  \n",
       "441 -0.039493 -0.004222  0.003064    57.0  \n",
       "\n",
       "[442 rows x 11 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8fe381e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>bmi</th>\n",
       "      <th>bp</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>s6</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>442.000</td>\n",
       "      <td>442.000</td>\n",
       "      <td>442.000</td>\n",
       "      <td>442.000</td>\n",
       "      <td>442.000</td>\n",
       "      <td>442.000</td>\n",
       "      <td>442.000</td>\n",
       "      <td>442.000</td>\n",
       "      <td>442.000</td>\n",
       "      <td>442.000</td>\n",
       "      <td>442.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>152.133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.048</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.048</td>\n",
       "      <td>77.093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>-0.112</td>\n",
       "      <td>-0.127</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.076</td>\n",
       "      <td>-0.126</td>\n",
       "      <td>-0.138</td>\n",
       "      <td>25.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.037</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>-0.034</td>\n",
       "      <td>-0.037</td>\n",
       "      <td>-0.034</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>-0.039</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>87.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.005</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>140.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.038</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.028</td>\n",
       "      <td>211.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.111</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.199</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.185</td>\n",
       "      <td>0.134</td>\n",
       "      <td>0.136</td>\n",
       "      <td>346.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           age      sex      bmi       bp       s1       s2       s3       s4  \\\n",
       "count  442.000  442.000  442.000  442.000  442.000  442.000  442.000  442.000   \n",
       "mean    -0.000    0.000   -0.000   -0.000   -0.000    0.000   -0.000   -0.000   \n",
       "std      0.048    0.048    0.048    0.048    0.048    0.048    0.048    0.048   \n",
       "min     -0.107   -0.045   -0.090   -0.112   -0.127   -0.116   -0.102   -0.076   \n",
       "25%     -0.037   -0.045   -0.034   -0.037   -0.034   -0.030   -0.035   -0.039   \n",
       "50%      0.005   -0.045   -0.007   -0.006   -0.004   -0.004   -0.007   -0.003   \n",
       "75%      0.038    0.051    0.031    0.036    0.028    0.030    0.029    0.034   \n",
       "max      0.111    0.051    0.171    0.132    0.154    0.199    0.181    0.185   \n",
       "\n",
       "            s5       s6   target  \n",
       "count  442.000  442.000  442.000  \n",
       "mean     0.000    0.000  152.133  \n",
       "std      0.048    0.048   77.093  \n",
       "min     -0.126   -0.138   25.000  \n",
       "25%     -0.033   -0.033   87.000  \n",
       "50%     -0.002   -0.001  140.500  \n",
       "75%      0.032    0.028  211.500  \n",
       "max      0.134    0.136  346.000  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(data.frame.describe(),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b3e48c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjIAAAHHCAYAAACle7JuAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA/v0lEQVR4nO3deVxV1f7/8fdBmRwARyYRcUicTS0lx6sUmpkmOWWF5jVvoeVQKZljg1Y3h8qhwasNmmmlZplmjtdErmNmllMopIKpAU4cDdbvj36eb0dAkcDDttfz8diPh3vtddb5nOVB3u699jk2Y4wRAACABbm5ugAAAICCIsgAAADLIsgAAADLIsgAAADLIsgAAADLIsgAAADLIsgAAADLIsgAAADLIsgAAADLIsgAKHL9+vVTtWrVXF2Gw+HDh2Wz2TRv3jxH2/jx42Wz2W7I87dr107t2rVz7K9fv142m02ffPLJDXn+4vb3AfwVBBngOthstnxt69evd3WpTjZv3qzx48crLS3N1aXcVI4dO6bx48dr165dri4lh+JcG1CYSrq6AMBKPvjgA6f9999/X6tXr87RXqdOnRtZ1jVt3rxZEyZMUL9+/eTn5+fqcoql5557TqNGjbquxxw7dkwTJkxQtWrV1Lhx43w/7uuvv77O6q7f1Wp75513lJ2dXeQ1ADcCQQa4Dg8++KDT/pYtW7R69eoc7QVhjFFmZqa8vb3/8li4fiVLllTJkkX7T+L58+dVqlQpeXh4FOnzXIu7u7tLnx8oTFxaAgrZ3Llz1b59e1WuXFmenp6qW7euZs2alaNftWrVdM8992jVqlVq1qyZvL299dZbb0mSjhw5onvvvVelS5dW5cqVNWzYMK1atSrXy1YJCQnq2LGjfH19VapUKbVt21bffvut4/j48eP19NNPS5LCwsIcl78OHz6ca/2DBw9WmTJldP78+RzH+vTpo4CAAGVlZUmSli1bps6dOysoKEienp6qUaOGnn/+ecfxvFxeE3Lla8lt7Yok/fTTT7r//vtVvnx5eXl5qVmzZvr888+v+hyXpaWlqV+/fvL19ZWfn59iYmJyvcSW2xqZ1atXq1WrVvLz81OZMmVUu3ZtPfvss47XcNttt0mS+vfv75jXy7W3a9dO9evX1/bt29WmTRuVKlXK8dgr18hclpWVpWeffVYBAQEqXbq07r33XiUnJzv1qVatmvr165fjsX8e81q15bZG5ty5cxoxYoRCQkLk6emp2rVr69///reMMU79bDabBg8erKVLl6p+/fry9PRUvXr1tHLlyhw1ATcCZ2SAQjZr1izVq1dP9957r0qWLKnly5fr8ccfV3Z2tmJjY5367tu3T3369NGgQYM0cOBA1a5dW+fOnVP79u11/PhxPfnkkwoICNCCBQu0bt26HM+1du1aderUSU2bNtW4cePk5ubmCFL//e9/dfvtt6t79+7av3+/PvroI02dOlUVK1aUJFWqVCnX+nv16qUZM2boyy+/VI8ePRzt58+f1/Lly9WvXz+VKFFCkjRv3jyVKVNGw4cPV5kyZbR27VqNHTtWGRkZevXVVwtlPn/44Qe1bNlSwcHBGjVqlEqXLq1FixapW7du+vTTT3Xffffl+VhjjLp27apNmzbpX//6l+rUqaMlS5YoJiYmX897zz33qGHDhpo4caI8PT118OBBR0isU6eOJk6cqLFjx+rRRx9V69atJUl33HGHY4xTp06pU6dO6t27tx588EH5+/tf9TlffPFF2Ww2jRw5UidOnNC0adMUGRmpXbt2XdeZuvzU9mfGGN17771at26dBgwYoMaNG2vVqlV6+umndfToUU2dOtWp/6ZNm/TZZ5/p8ccfV9myZfX6668rOjpaSUlJqlChQr7rBAqFAVBgsbGx5sofo/Pnz+foFxUVZapXr+7UFhoaaiSZlStXOrW/9tprRpJZunSpo+3ChQsmPDzcSDLr1q0zxhiTnZ1tatWqZaKiokx2drbT84eFhZk777zT0fbqq68aSSYxMfGaryk7O9sEBweb6Ohop/ZFixYZSWbjxo1Xfa2DBg0ypUqVMpmZmY62mJgYExoa6thft26d02u5LDEx0Ugyc+fOdbR16NDBNGjQwGm87Oxsc8cdd5hatWpd9bUsXbrUSDKvvPKKo+333383rVu3zvE848aNc/q7nDp1qpFkfv311zzH37p1a45xLmvbtq2RZGbPnp3rsbZt2zr2L89HcHCwycjIcLRfnvPp06c72kJDQ01MTMw1x7xabVf+fVyepxdeeMGp3/33329sNps5ePCgo02S8fDwcGr77rvvjCTzxhtv5HguoKhxaQkoZH/+n3N6erpOnjyptm3b6ueff1Z6erpT37CwMEVFRTm1rVy5UsHBwbr33nsdbV5eXho4cKBTv127dunAgQN64IEHdOrUKZ08eVInT57UuXPn1KFDB23cuLFACzptNpt69OihFStW6OzZs472jz/+WMHBwWrVqlWur/XMmTM6efKkWrdurfPnz+unn3667ue+0unTp7V27Vr17NnTMf7Jkyd16tQpRUVF6cCBAzp69Giej1+xYoVKliypxx57zNFWokQJDRky5JrPfXlR9LJlywq8MNbT01P9+/fPd/+HH35YZcuWdezff//9CgwM1IoVKwr0/Pm1YsUKlShRQk888YRT+4gRI2SM0VdffeXUHhkZqRo1ajj2GzZsKB8fH/38889FWieQG4IMUMi+/fZbRUZGqnTp0vLz81OlSpUcayNyCzJXOnLkiGrUqJFjvUbNmjWd9g8cOCBJiomJUaVKlZy2d999V3a7Pcfz5VevXr104cIFxzqUs2fPasWKFerRo4dTXT/88IPuu+8++fr6ysfHR5UqVXIsfC7oc//ZwYMHZYzRmDFjcrzGcePGSZJOnDiR5+OPHDmiwMBAlSlTxqm9du3a13zuXr16qWXLlvrnP/8pf39/9e7dW4sWLbquUBMcHHxdC3tr1arltG+z2VSzZs081zMVliNHjigoKMgpREn/d/fdkSNHnNqrVq2aY4xy5crpt99+K7oigTywRgYoRIcOHVKHDh0UHh6uKVOmKCQkRB4eHlqxYoWmTp2a45fgX7lD6fJYr776ap63/l75Czy/WrRooWrVqmnRokV64IEHtHz5cl24cEG9evVy9ElLS1Pbtm3l4+OjiRMnqkaNGvLy8tKOHTs0cuTIq/7Cz+uD565cJHx5jKeeeirHmavLrgx4hcXb21sbN27UunXr9OWXX2rlypX6+OOP1b59e3399deOdULXGqOwXW3u8lNTYcjrecwVC4OBG4EgAxSi5cuXy2636/PPP3f6X2tuC3XzEhoaqr1798oY4/RL6+DBg079Lp/a9/HxUWRk5FXHLMgn1vbs2VPTp09XRkaGPv74Y1WrVk0tWrRwHF+/fr1OnTqlzz77TG3atHG0JyYmXnPscuXKSVKOu4eu/J9/9erVJf1xu/C1XmNuQkNDtWbNGp09e9Yp1O3bty9fj3dzc1OHDh3UoUMHTZkyRS+99JJGjx6tdevWKTIystA/CfjyWbbLjDE6ePCgGjZs6GgrV65crnddHTlyxDFf0vX9nYeGhuqbb77RmTNnnM7KXL48GBoamu+xgBuNS0tAIbr8P9U//880PT1dc+fOzfcYUVFROnr0qNPtxZmZmXrnnXec+jVt2lQ1atTQv//9b6e1LJf9+uuvjj+XLl1aUs7gcDW9evWS3W7Xe++9p5UrV6pnz55Ox3N7rRcvXtTMmTOvOXZoaKhKlCihjRs3OrVf+djKlSurXbt2euutt3T8+PEc4/z5Nebm7rvv1u+//+50+3tWVpbeeOONa9Z4+vTpHG2Xz3zZ7XZJBZvXq3n//fd15swZx/4nn3yi48ePq1OnTo62GjVqaMuWLbp48aKj7Ysvvshxm/b11Hb33XcrKytLb775plP71KlTZbPZnJ4fKG44IwMUorvuukseHh7q0qWLBg0apLNnz+qdd95R5cqVc/1FnJtBgwbpzTffVJ8+ffTkk08qMDBQ8+fPl5eXl6T/+5+2m5ub3n33XXXq1En16tVT//79FRwcrKNHj2rdunXy8fHR8uXLJf0ReiRp9OjR6t27t9zd3dWlSxfHL7vcNGnSRDVr1tTo0aNlt9udLitJf9zKW65cOcXExOiJJ56QzWbTBx98kK/LC76+vurRo4feeOMN2Ww21ahRQ1988UWu611mzJihVq1aqUGDBho4cKCqV6+u1NRUxcfH65dfftF3332X5/N06dJFLVu21KhRo3T48GHVrVtXn332Wb7W70ycOFEbN25U586dFRoaqhMnTmjmzJmqUqWKY8FzjRo15Ofnp9mzZ6ts2bIqXbq0mjdvnuvap/woX768WrVqpf79+ys1NVXTpk1TzZo1nRZ6//Of/9Qnn3yijh07qmfPnjp06JA+/PBDp8W311tbly5d9I9//EOjR4/W4cOH1ahRI3399ddatmyZhg4dmmNsoFhx3Q1TgPXldvv1559/bho2bGi8vLxMtWrVzMsvv2z+85//5Lj9OTQ01HTu3DnXcX/++WfTuXNn4+3tbSpVqmRGjBhhPv30UyPJbNmyxanvzp07Tffu3U2FChWMp6enCQ0NNT179jRr1qxx6vf888+b4OBg4+bmlu9bsUePHm0kmZo1a+Z6/NtvvzUtWrQw3t7eJigoyDzzzDNm1apVOW6tvvJ2X2OM+fXXX010dLQpVaqUKVeunBk0aJDZs2dPrrcMHzp0yDz88MMmICDAuLu7m+DgYHPPPfeYTz755Jqv4dSpU+ahhx4yPj4+xtfX1zz00ENm586d17z9es2aNaZr164mKCjIeHh4mKCgINOnTx+zf/9+p/GXLVtm6tata0qWLOk0Ztu2bU29evVyrSmv268/+ugjExcXZypXrmy8vb1N586dzZEjR3I8/rXXXjPBwcHG09PTtGzZ0mzbti3HmFerLbe/jzNnzphhw4aZoKAg4+7ubmrVqmVeffVVp1v7jfnj9uvY2NgcNeV1WzhQ1GzGsDoLsIJp06Zp2LBh+uWXXxQcHOzqcgCgWCDIAMXQhQsXnO54yczM1K233qqsrCzt37/fhZUBQPHCGhmgGOrevbuqVq2qxo0bKz09XR9++KF++uknzZ8/39WlAUCxQpABiqGoqCi9++67mj9/vrKyslS3bl0tXLgwx4JbAPi749ISAACwLD5HBgAAWBZBBgAAWNZNv0YmOztbx44dU9myZQv948QBAEDRMMbozJkzCgoKkptb3uddbvogc+zYMYWEhLi6DAAAUADJycmqUqVKnsdv+iBz+QvQkpOT5ePj4+JqAABAfmRkZCgkJMTpi0xzc9MHmcuXk3x8fAgyAABYzLWWhbDYFwAAWBZBBgAAWBZBBgAAWBZBBgAAWBZBBgAAWBZBBgAAWBZBBgAAWBZBBgAAWBZBBgAAWBZBBgAAWJZLg0xWVpbGjBmjsLAweXt7q0aNGnr++edljHH0McZo7NixCgwMlLe3tyIjI3XgwAEXVg0AAIoLlwaZl19+WbNmzdKbb76pH3/8US+//LJeeeUVvfHGG44+r7zyil5//XXNnj1bCQkJKl26tKKiopSZmenCygEAQHFgM38+/XGD3XPPPfL399ecOXMcbdHR0fL29taHH34oY4yCgoI0YsQIPfXUU5Kk9PR0+fv7a968eerdu/c1nyMjI0O+vr5KT0/nSyMBALCI/P7+dukZmTvuuENr1qzR/v37JUnfffedNm3apE6dOkmSEhMTlZKSosjISMdjfH191bx5c8XHx7ukZgAAUHyUdOWTjxo1ShkZGQoPD1eJEiWUlZWlF198UX379pUkpaSkSJL8/f2dHufv7+84diW73S673e7Yz8jIKKLqAQCAq7k0yCxatEjz58/XggULVK9ePe3atUtDhw5VUFCQYmJiCjTmpEmTNGHChEKu9OZSbdSXri7hmg5P7uzqEgAAFuDSS0tPP/20Ro0apd69e6tBgwZ66KGHNGzYME2aNEmSFBAQIElKTU11elxqaqrj2JXi4uKUnp7u2JKTk4v2RQAAAJdxaZA5f/683NycSyhRooSys7MlSWFhYQoICNCaNWscxzMyMpSQkKCIiIhcx/T09JSPj4/TBgAAbk4uvbTUpUsXvfjii6patarq1aunnTt3asqUKXrkkUckSTabTUOHDtULL7ygWrVqKSwsTGPGjFFQUJC6devmytIBAEAx4NIg88Ybb2jMmDF6/PHHdeLECQUFBWnQoEEaO3aso88zzzyjc+fO6dFHH1VaWppatWqllStXysvLy4WVAwCA4sClnyNzI/A5Mjmx2BcAUNxZ4nNkAAAA/gqCDAAAsCyCDAAAsCyCDAAAsCyCDAAAsCyCDAAAsCyCDAAAsCyCDAAAsCyCDAAAsCyCDAAAsCyCDAAAsCyCDAAAsCyCDAAAsCyCDAAAsCyCDAAAsCyCDAAAsCyCDAAAsCyCDAAAsCyCDAAAsCyCDAAAsCyCDAAAsCyCDAAAsCyCDAAAsCyCDAAAsCyCDAAAsCyCDAAAsCyCDAAAsCyCDAAAsCyCDAAAsCyCDAAAsCyCDAAAsCyCDAAAsCyCDAAAsCyXBplq1arJZrPl2GJjYyVJmZmZio2NVYUKFVSmTBlFR0crNTXVlSUDAIBixKVBZuvWrTp+/LhjW716tSSpR48ekqRhw4Zp+fLlWrx4sTZs2KBjx46pe/furiwZAAAUIyVd+eSVKlVy2p88ebJq1Kihtm3bKj09XXPmzNGCBQvUvn17SdLcuXNVp04dbdmyRS1atHBFyQAAoBgpNmtkLl68qA8//FCPPPKIbDabtm/frkuXLikyMtLRJzw8XFWrVlV8fLwLKwUAAMWFS8/I/NnSpUuVlpamfv36SZJSUlLk4eEhPz8/p37+/v5KSUnJcxy73S673e7Yz8jIKIpyAQBAMVBszsjMmTNHnTp1UlBQ0F8aZ9KkSfL19XVsISEhhVQhAAAobopFkDly5Ii++eYb/fOf/3S0BQQE6OLFi0pLS3Pqm5qaqoCAgDzHiouLU3p6umNLTk4uqrIBAICLFYsgM3fuXFWuXFmdO3d2tDVt2lTu7u5as2aNo23fvn1KSkpSREREnmN5enrKx8fHaQMAADcnl6+Ryc7O1ty5cxUTE6OSJf+vHF9fXw0YMEDDhw9X+fLl5ePjoyFDhigiIoI7lgAAgKRiEGS++eYbJSUl6ZFHHslxbOrUqXJzc1N0dLTsdruioqI0c+ZMF1QJAACKI5sxxri6iKKUkZEhX19fpaenc5np/6s26ktXl3BNhyd3vnYnAMBNK7+/v4vFGhkAAICCIMgAAADLIsgAAADLcvliXyC/WNsDALgSZ2QAAIBlEWQAAIBlEWQAAIBlEWQAAIBlEWQAAIBlEWQAAIBlEWQAAIBlEWQAAIBlEWQAAIBlEWQAAIBlEWQAAIBlEWQAAIBlEWQAAIBlEWQAAIBlEWQAAIBlEWQAAIBlEWQAAIBlEWQAAIBlEWQAAIBlEWQAAIBlEWQAAIBlEWQAAIBlEWQAAIBlEWQAAIBlEWQAAIBllXR1AcDNrNqoL11dwjUdntzZ1SUAQIFxRgYAAFgWQQYAAFgWQQYAAFiWy4PM0aNH9eCDD6pChQry9vZWgwYNtG3bNsdxY4zGjh2rwMBAeXt7KzIyUgcOHHBhxQAAoLhwaZD57bff1LJlS7m7u+urr77S3r179dprr6lcuXKOPq+88opef/11zZ49WwkJCSpdurSioqKUmZnpwsoBAEBx4NK7ll5++WWFhIRo7ty5jrawsDDHn40xmjZtmp577jl17dpVkvT+++/L399fS5cuVe/evW94zQAAoPhw6RmZzz//XM2aNVOPHj1UuXJl3XrrrXrnnXccxxMTE5WSkqLIyEhHm6+vr5o3b674+HhXlAwAAIoRlwaZn3/+WbNmzVKtWrW0atUqPfbYY3riiSf03nvvSZJSUlIkSf7+/k6P8/f3dxy7kt1uV0ZGhtMGAABuTi69tJSdna1mzZrppZdekiTdeuut2rNnj2bPnq2YmJgCjTlp0iRNmDChMMsEAADFlEvPyAQGBqpu3bpObXXq1FFSUpIkKSAgQJKUmprq1Cc1NdVx7EpxcXFKT093bMnJyUVQOQAAKA5cGmRatmypffv2ObXt379foaGhkv5Y+BsQEKA1a9Y4jmdkZCghIUERERG5junp6SkfHx+nDQAA3Jxcemlp2LBhuuOOO/TSSy+pZ8+e+t///qe3335bb7/9tiTJZrNp6NCheuGFF1SrVi2FhYVpzJgxCgoKUrdu3VxZOgAAKAZcGmRuu+02LVmyRHFxcZo4caLCwsI0bdo09e3b19HnmWee0blz5/Too48qLS1NrVq10sqVK+Xl5eXCygEAQHHg8m+/vueee3TPPffkedxms2nixImaOHHiDawKAABYgcu/ogAAAKCgCDIAAMCyCDIAAMCyCDIAAMCyCDIAAMCyCDIAAMCyCDIAAMCyCDIAAMCyCDIAAMCyCDIAAMCyCDIAAMCyCDIAAMCyCDIAAMCyCDIAAMCyCDIAAMCyCDIAAMCyCDIAAMCyCDIAAMCyCDIAAMCyCDIAAMCyCDIAAMCyCDIAAMCyCDIAAMCyCDIAAMCyCDIAAMCyCDIAAMCyCDIAAMCyCDIAAMCyCDIAAMCyCDIAAMCyCDIAAMCyCDIAAMCyCDIAAMCyXBpkxo8fL5vN5rSFh4c7jmdmZio2NlYVKlRQmTJlFB0drdTUVBdWDAAAihOXn5GpV6+ejh8/7tg2bdrkODZs2DAtX75cixcv1oYNG3Ts2DF1797dhdUCAIDipKTLCyhZUgEBATna09PTNWfOHC1YsEDt27eXJM2dO1d16tTRli1b1KJFixtdKgAAKGZcfkbmwIEDCgoKUvXq1dW3b18lJSVJkrZv365Lly4pMjLS0Tc8PFxVq1ZVfHy8q8oFAADFiEvPyDRv3lzz5s1T7dq1dfz4cU2YMEGtW7fWnj17lJKSIg8PD/n5+Tk9xt/fXykpKXmOabfbZbfbHfsZGRlFVT4AAHAxlwaZTp06Of7csGFDNW/eXKGhoVq0aJG8vb0LNOakSZM0YcKEwioR+FuqNupLV5dwTYcnd3Z1CQCKAZdfWvozPz8/3XLLLTp48KACAgJ08eJFpaWlOfVJTU3NdU3NZXFxcUpPT3dsycnJRVw1AABwlWIVZM6ePatDhw4pMDBQTZs2lbu7u9asWeM4vm/fPiUlJSkiIiLPMTw9PeXj4+O0AQCAm5NLLy099dRT6tKli0JDQ3Xs2DGNGzdOJUqUUJ8+feTr66sBAwZo+PDhKl++vHx8fDRkyBBFRERwxxIAAJBUwCCzY8cOubu7q0GDBpKkZcuWae7cuapbt67Gjx8vDw+PfI3zyy+/qE+fPjp16pQqVaqkVq1aacuWLapUqZIkaerUqXJzc1N0dLTsdruioqI0c+bMgpQMAABuQgUKMoMGDdKoUaPUoEED/fzzz+rdu7fuu+8+LV68WOfPn9e0adPyNc7ChQuvetzLy0szZszQjBkzClImAAC4yRVojcz+/fvVuHFjSdLixYvVpk0bLViwQPPmzdOnn35amPUBAADkqUBBxhij7OxsSdI333yju+++W5IUEhKikydPFl51AAAAV1GgINOsWTO98MIL+uCDD7RhwwZ17vzH5zkkJibK39+/UAsEAADIS4GCzLRp07Rjxw4NHjxYo0ePVs2aNSVJn3zyie64445CLRAAACAvBVrs27BhQ33//fc52l999VWVKFHiLxcFAACQHwX+QLy0tDS9++67iouL0+nTpyVJe/fu1YkTJwqtOAAAgKsp0BmZ3bt3q0OHDvLz89Phw4c1cOBAlS9fXp999pmSkpL0/vvvF3adAAAAORTojMzw4cPVv39/HThwQF5eXo72u+++Wxs3biy04gAAAK6mQEFm69atGjRoUI724OBgpaSk/OWiAAAA8qNAQcbT01MZGRk52vfv3+/4egEAAICiVqAgc++992rixIm6dOmSJMlmsykpKUkjR45UdHR0oRYIAACQlwIFmddee01nz55V5cqVdeHCBbVt21Y1a9ZU2bJl9eKLLxZ2jQAAALkq0F1Lvr6+Wr16tTZt2qTdu3fr7NmzatKkiSIjIwu7PgAAgDwVKMhc1qpVK7Vq1aqwagEAALgu+Q4yr7/+er4HfeKJJwpUDAAAwPXId5CZOnVqvvrZbDaCDAAAuCHyHWQSExOLsg4AAIDrVuDvWrrMGCNjTGHUAgAAcF0KHGTmzJmj+vXry8vLS15eXqpfv77efffdwqwNAADgqgp019LYsWM1ZcoUDRkyRBEREZKk+Ph4DRs2TElJSZo4cWKhFgkAAJCbAgWZWbNm6Z133lGfPn0cbffee68aNmyoIUOGEGQAAMANUaBLS5cuXVKzZs1ytDdt2lS///77Xy4KAAAgPwoUZB566CHNmjUrR/vbb7+tvn37/uWiAAAA8qPAn+w7Z84cff3112rRooUkKSEhQUlJSXr44Yc1fPhwR78pU6b89SoBAAByUaAgs2fPHjVp0kSSdOjQIUlSxYoVVbFiRe3Zs8fRz2azFUKJAAAAuStQkFm3bl1h1wEAAHDd/vIH4gEAALhKgc7IZGZm6o033tC6det04sQJZWdnOx3fsWNHoRQHAABwNQUKMgMGDNDXX3+t+++/X7fffjtrYQAAgEsUKMh88cUXWrFihVq2bFnY9QAAirlqo750dQnXdHhyZ1eXgBukQGtkgoODVbZs2cKuBQAA4LoUKMi89tprGjlypI4cOVLY9QAAAORbgS4tNWvWTJmZmapevbpKlSold3d3p+OnT58ulOIAAACupkBBpk+fPjp69Kheeukl+fv7F8pi38mTJysuLk5PPvmkpk2bJumPu6NGjBihhQsXym63KyoqSjNnzpS/v/9ffj4ANxfWbQB/TwUKMps3b1Z8fLwaNWpUKEVs3bpVb731lho2bOjUPmzYMH355ZdavHixfH19NXjwYHXv3l3ffvttoTwvAACwtgKtkQkPD9eFCxcKpYCzZ8+qb9++euedd1SuXDlHe3p6uubMmaMpU6aoffv2atq0qebOnavNmzdry5YthfLcAADA2goUZCZPnqwRI0Zo/fr1OnXqlDIyMpy26xEbG6vOnTsrMjLSqX379u26dOmSU3t4eLiqVq2q+Pj4gpQNAABuMgW6tNSxY0dJUocOHZzajTGy2WzKysrK1zgLFy7Ujh07tHXr1hzHUlJS5OHhIT8/P6d2f39/paSk5Dmm3W6X3W537F9vsAIAANbhsi+NTE5O1pNPPqnVq1fLy8vrL4932aRJkzRhwoRCGw8AABRfBQoybdu2/ctPvH37dp04cUJNmjRxtGVlZWnjxo168803tWrVKl28eFFpaWlOZ2VSU1MVEBCQ57hxcXEaPny4Yz8jI0MhISF/uV4AAFD8FCjIXHb+/HklJSXp4sWLTu1X3n2Umw4dOuj77793auvfv7/Cw8M1cuRIhYSEyN3dXWvWrFF0dLQkad++fUpKSlJERESe43p6esrT07MArwYAAFhNgYLMr7/+qv79++urr77K9Xh+1siULVtW9evXd2orXbq0KlSo4GgfMGCAhg8frvLly8vHx0dDhgxRRESEWrRoUZCyAQDATaZAdy0NHTpUaWlpSkhIkLe3t1auXKn33ntPtWrV0ueff15oxU2dOlX33HOPoqOj1aZNGwUEBOizzz4rtPEBAIC1FeiMzNq1a7Vs2TI1a9ZMbm5uCg0N1Z133ikfHx9NmjRJnTsX7NMr169f77Tv5eWlGTNmaMaMGQUaDwAA3NwKdEbm3Llzqly5siSpXLly+vXXXyVJDRo00I4dOwqvOgAAgKsoUJCpXbu29u3bJ0lq1KiR3nrrLR09elSzZ89WYGBgoRYIAACQlwJdWnryySd1/PhxSdK4cePUsWNHzZ8/Xx4eHpo3b15h1gcAAJCnAgWZBx980PHnpk2b6siRI/rpp59UtWpVVaxYsdCKK+74tl0AfxX/jgB/TYEuLV3J09NTbm5uKlGiRGEMBwAAkC8Fvv16zpw5kv74zJg2bdqoSZMmCgkJyXHnEQAAQFEpUJD55JNP1KhRI0nS8uXLdfjwYf30008aNmyYRo8eXagFAgAA5KVAQebkyZOO7ztasWKFevTooVtuuUWPPPJIjq8dAAAAKCoFCjL+/v7au3evsrKytHLlSt15552S/vjuJdbJAACAG6VAdy31799fPXv2VGBgoGw2myIjIyVJCQkJCg8PL9QCAQAA8lKgIDN+/Hg1aNBASUlJ6tGjh+PbpkuUKKFRo0YVaoEAAAB5KVCQkaR169Zp4sSJKl++vKMtJiamUIoCAADIj+taI/PLL784/rxgwQKdPXtW0h/fsZScnFy4lQEAAFzDdZ2RCQ8PV4UKFdSyZUtlZmYqOTlZVatW1eHDh3Xp0qWiqhEAACBX13VGJi0tTYsXL1bTpk2VnZ2tu+++W7fccovsdrtWrVql1NTUoqoTAAAgh+sKMpcuXdLtt9+uESNGyNvbWzt37tTcuXNVokQJ/ec//1FYWJhq165dVLUCAAA4ua5LS35+fmrcuLFatmypixcv6sKFC2rZsqVKliypjz/+WMHBwdq6dWtR1QoAAODkus7IHD16VM8995w8PT31+++/q2nTpmrdurUuXryoHTt2yGazqVWrVkVVKwAAgJPrCjIVK1ZUly5dNGnSJJUqVUpbt27VkCFDZLPZ9NRTT8nX11dt27YtqloBAACcFOgrCi7z9fVVz5495e7urrVr1yoxMVGPP/54YdUGAABwVQX+QLzdu3crODhYkhQaGip3d3cFBASoV69ehVYcAADA1RQ4yISEhDj+vGfPnkIpBgAA4Hr8pUtLAAAArkSQAQAAlkWQAQAAlkWQAQAAlkWQAQAAlkWQAQAAlkWQAQAAlkWQAQAAlkWQAQAAlkWQAQAAlkWQAQAAluXSIDNr1iw1bNhQPj4+8vHxUUREhL766ivH8czMTMXGxqpChQoqU6aMoqOjlZqa6sKKAQBAceLSIFOlShVNnjxZ27dv17Zt29S+fXt17dpVP/zwgyRp2LBhWr58uRYvXqwNGzbo2LFj6t69uytLBgAAxUiBv/26MHTp0sVp/8UXX9SsWbO0ZcsWValSRXPmzNGCBQvUvn17SdLcuXNVp04dbdmyRS1atHBFyQAAoBgpNmtksrKytHDhQp07d04RERHavn27Ll26pMjISEef8PBwVa1aVfHx8S6sFAAAFBcuPSMjSd9//70iIiKUmZmpMmXKaMmSJapbt6527dolDw8P+fn5OfX39/dXSkpKnuPZ7XbZ7XbHfkZGRlGVDgAAXMzlZ2Rq166tXbt2KSEhQY899phiYmK0d+/eAo83adIk+fr6OraQkJBCrBYAABQnLg8yHh4eqlmzppo2bapJkyapUaNGmj59ugICAnTx4kWlpaU59U9NTVVAQECe48XFxSk9Pd2xJScnF/ErAAAAruLyIHOl7Oxs2e12NW3aVO7u7lqzZo3j2L59+5SUlKSIiIg8H+/p6em4nfvyBgAAbk4uXSMTFxenTp06qWrVqjpz5owWLFig9evXa9WqVfL19dWAAQM0fPhwlS9fXj4+PhoyZIgiIiK4YwkAAEhycZA5ceKEHn74YR0/fly+vr5q2LChVq1apTvvvFOSNHXqVLm5uSk6Olp2u11RUVGaOXOmK0sGAADFiEuDzJw5c6563MvLSzNmzNCMGTNuUEUAAMBKit0aGQAAgPwiyAAAAMsiyAAAAMsiyAAAAMsiyAAAAMsiyAAAAMsiyAAAAMsiyAAAAMsiyAAAAMsiyAAAAMsiyAAAAMsiyAAAAMsiyAAAAMsiyAAAAMsiyAAAAMsiyAAAAMsiyAAAAMsiyAAAAMsiyAAAAMsiyAAAAMsiyAAAAMsiyAAAAMsiyAAAAMsiyAAAAMsiyAAAAMsiyAAAAMsiyAAAAMsiyAAAAMsiyAAAAMsiyAAAAMsiyAAAAMsiyAAAAMsiyAAAAMtyaZCZNGmSbrvtNpUtW1aVK1dWt27dtG/fPqc+mZmZio2NVYUKFVSmTBlFR0crNTXVRRUDAIDixKVBZsOGDYqNjdWWLVu0evVqXbp0SXfddZfOnTvn6DNs2DAtX75cixcv1oYNG3Ts2DF1797dhVUDAIDioqQrn3zlypVO+/PmzVPlypW1fft2tWnTRunp6ZozZ44WLFig9u3bS5Lmzp2rOnXqaMuWLWrRooUrygYAAMVEsVojk56eLkkqX768JGn79u26dOmSIiMjHX3Cw8NVtWpVxcfHu6RGAABQfLj0jMyfZWdna+jQoWrZsqXq168vSUpJSZGHh4f8/Pyc+vr7+yslJSXXcex2u+x2u2M/IyOjyGoGAACuVWzOyMTGxmrPnj1auHDhXxpn0qRJ8vX1dWwhISGFVCEAAChuikWQGTx4sL744gutW7dOVapUcbQHBATo4sWLSktLc+qfmpqqgICAXMeKi4tTenq6Y0tOTi7K0gEAgAu5NMgYYzR48GAtWbJEa9euVVhYmNPxpk2byt3dXWvWrHG07du3T0lJSYqIiMh1TE9PT/n4+DhtAADg5uTSNTKxsbFasGCBli1bprJlyzrWvfj6+srb21u+vr4aMGCAhg8frvLly8vHx0dDhgxRREQEdywBAADXBplZs2ZJktq1a+fUPnfuXPXr10+SNHXqVLm5uSk6Olp2u11RUVGaOXPmDa4UAAAURy4NMsaYa/bx8vLSjBkzNGPGjBtQEQAAsJJisdgXAACgIAgyAADAsggyAADAsggyAADAsggyAADAsggyAADAsggyAADAsorNt18DAFCUqo360tUlXNPhyZ1dXYLlcEYGAABYFkEGAABYFkEGAABYFkEGAABYFkEGAABYFkEGAABYFkEGAABYFkEGAABYFkEGAABYFkEGAABYFkEGAABYFkEGAABYFkEGAABYFkEGAABYFkEGAABYFkEGAABYFkEGAABYFkEGAABYFkEGAABYFkEGAABYFkEGAABYFkEGAABYFkEGAABYFkEGAABYFkEGAABYlkuDzMaNG9WlSxcFBQXJZrNp6dKlTseNMRo7dqwCAwPl7e2tyMhIHThwwDXFAgCAYselQebcuXNq1KiRZsyYkevxV155Ra+//rpmz56thIQElS5dWlFRUcrMzLzBlQIAgOKopCufvFOnTurUqVOux4wxmjZtmp577jl17dpVkvT+++/L399fS5cuVe/evW9kqQAAoBgqtmtkEhMTlZKSosjISEebr6+vmjdvrvj4eBdWBgAAiguXnpG5mpSUFEmSv7+/U7u/v7/jWG7sdrvsdrtjPyMjo2gKBAAALldsz8gU1KRJk+Tr6+vYQkJCXF0SAAAoIsU2yAQEBEiSUlNTndpTU1Mdx3ITFxen9PR0x5acnFykdQIAANcptkEmLCxMAQEBWrNmjaMtIyNDCQkJioiIyPNxnp6e8vHxcdoAAMDNyaVrZM6ePauDBw869hMTE7Vr1y6VL19eVatW1dChQ/XCCy+oVq1aCgsL05gxYxQUFKRu3bq5rmgAAFBsuDTIbNu2Tf/4xz8c+8OHD5ckxcTEaN68eXrmmWd07tw5Pfroo0pLS1OrVq20cuVKeXl5uapkAABQjLg0yLRr107GmDyP22w2TZw4URMnTryBVQEAAKsotmtkAAAAroUgAwAALIsgAwAALIsgAwAALIsgAwAALIsgAwAALIsgAwAALIsgAwAALIsgAwAALIsgAwAALIsgAwAALMul37UEAADyVm3Ul64u4ZoOT+7s0ufnjAwAALAsggwAALAsggwAALAsggwAALAsggwAALAsggwAALAsggwAALAsggwAALAsggwAALAsggwAALAsggwAALAsggwAALAsggwAALAsggwAALAsggwAALAsggwAALAsggwAALAsggwAALAsggwAALAsggwAALAsggwAALAsSwSZGTNmqFq1avLy8lLz5s31v//9z9UlAQCAYqDYB5mPP/5Yw4cP17hx47Rjxw41atRIUVFROnHihKtLAwAALlbsg8yUKVM0cOBA9e/fX3Xr1tXs2bNVqlQp/ec//3F1aQAAwMWKdZC5ePGitm/frsjISEebm5ubIiMjFR8f78LKAABAcVDS1QVczcmTJ5WVlSV/f3+ndn9/f/3000+5PsZut8tutzv209PTJUkZGRmFXl+2/Xyhj1nYcnvd1F10rqzbijVL1qzbijVL1qzbijVL1qzbijUX9rjGmKt3NMXY0aNHjSSzefNmp/ann37a3H777bk+Zty4cUYSGxsbGxsb202wJScnXzUrFOszMhUrVlSJEiWUmprq1J6amqqAgIBcHxMXF6fhw4c79rOzs3X69GlVqFBBNputSOt1hYyMDIWEhCg5OVk+Pj6uLqdYYW7yxtzkjnnJG3OTN+Ymb39lbowxOnPmjIKCgq7ar1gHGQ8PDzVt2lRr1qxRt27dJP0RTNasWaPBgwfn+hhPT095eno6tfn5+RVxpa7n4+PDD1AemJu8MTe5Y17yxtzkjbnJW0HnxtfX95p9inWQkaThw4crJiZGzZo10+23365p06bp3Llz6t+/v6tLAwAALlbsg0yvXr3066+/auzYsUpJSVHjxo21cuXKHAuAAQDA30+xDzKSNHjw4DwvJf3deXp6aty4cTkup4G5uRrmJnfMS96Ym7wxN3m7EXNjM+Za9zUBAAAUT8X6A/EAAACuhiADAAAsiyADAAAsiyADAAAsiyBjAePHj5fNZnPawsPDHcczMzMVGxurChUqqEyZMoqOjs7xacg3i40bN6pLly4KCgqSzWbT0qVLnY4bYzR27FgFBgbK29tbkZGROnDggFOf06dPq2/fvvLx8ZGfn58GDBigs2fP3sBXUTSuNTf9+vXL8T7q2LGjU5+bcW4mTZqk2267TWXLllXlypXVrVs37du3z6lPfn6GkpKS1LlzZ5UqVUqVK1fW008/rd9///1GvpRCl5+5adeuXY73zb/+9S+nPjfj3MyaNUsNGzZ0fJBbRESEvvrqK8fxv+t7Rrr23Nzo9wxBxiLq1aun48ePO7ZNmzY5jg0bNkzLly/X4sWLtWHDBh07dkzdu3d3YbVF59y5c2rUqJFmzJiR6/FXXnlFr7/+umbPnq2EhASVLl1aUVFRyszMdPTp27evfvjhB61evVpffPGFNm7cqEcfffRGvYQic625kaSOHTs6vY8++ugjp+M349xs2LBBsbGx2rJli1avXq1Lly7prrvu0rlz5xx9rvUzlJWVpc6dO+vixYvavHmz3nvvPc2bN09jx451xUsqNPmZG0kaOHCg0/vmlVdecRy7WeemSpUqmjx5srZv365t27apffv26tq1q3744QdJf9/3jHTtuZFu8HumUL7dEUVq3LhxplGjRrkeS0tLM+7u7mbx4sWOth9//NFIMvHx8TeoQteQZJYsWeLYz87ONgEBAebVV191tKWlpRlPT0/z0UcfGWOM2bt3r5Fktm7d6ujz1VdfGZvNZo4ePXrDai9qV86NMcbExMSYrl275vmYv8vcnDhxwkgyGzZsMMbk72doxYoVxs3NzaSkpDj6zJo1y/j4+Bi73X5jX0ARunJujDGmbdu25sknn8zzMX+XuTHGmHLlypl3332X90wuLs+NMTf+PcMZGYs4cOCAgoKCVL16dfXt21dJSUmSpO3bt+vSpUuKjIx09A0PD1fVqlUVHx/vqnJdIjExUSkpKU5z4evrq+bNmzvmIj4+Xn5+fmrWrJmjT2RkpNzc3JSQkHDDa77R1q9fr8qVK6t27dp67LHHdOrUKcexv8vcpKenS5LKly8vKX8/Q/Hx8WrQoIHTJ4pHRUUpIyPD6X+hVnfl3Fw2f/58VaxYUfXr11dcXJzOnz/vOPZ3mJusrCwtXLhQ586dU0REBO+ZP7lybi67ke8ZS3yy799d8+bNNW/ePNWuXVvHjx/XhAkT1Lp1a+3Zs0cpKSny8PDI8cWY/v7+SklJcU3BLnL59V759RV/nouUlBRVrlzZ6XjJkiVVvnz5m36+OnbsqO7duyssLEyHDh3Ss88+q06dOik+Pl4lSpT4W8xNdna2hg4dqpYtW6p+/fqSlK+foZSUlFzfV5eP3QxymxtJeuCBBxQaGqqgoCDt3r1bI0eO1L59+/TZZ59Jurnn5vvvv1dERIQyMzNVpkwZLVmyRHXr1tWuXbv+9u+ZvOZGuvHvGYKMBXTq1Mnx54YNG6p58+YKDQ3VokWL5O3t7cLKYCW9e/d2/LlBgwZq2LChatSoofXr16tDhw4urOzGiY2N1Z49e5zWmOEPec3Nn9dINWjQQIGBgerQoYMOHTqkGjVq3Ogyb6jatWtr165dSk9P1yeffKKYmBht2LDB1WUVC3nNTd26dW/4e4ZLSxbk5+enW265RQcPHlRAQIAuXryotLQ0pz6pqakKCAhwTYEucvn1XnnnwJ/nIiAgQCdOnHA6/vvvv+v06dN/u/mqXr26KlasqIMHD0q6+edm8ODB+uKLL7Ru3TpVqVLF0Z6fn6GAgIBc31eXj1ldXnOTm+bNm0uS0/vmZp0bDw8P1axZU02bNtWkSZPUqFEjTZ8+nfeM8p6b3BT1e4YgY0Fnz57VoUOHFBgYqKZNm8rd3V1r1qxxHN+3b5+SkpKcrlf+HYSFhSkgIMBpLjIyMpSQkOCYi4iICKWlpWn79u2OPmvXrlV2drbjh+3v4pdfftGpU6cUGBgo6eadG2OMBg8erCVLlmjt2rUKCwtzOp6fn6GIiAh9//33TkFv9erV8vHxcZxOt6JrzU1udu3aJUlO75ubcW5yk52dLbvd/rd+z+Tl8tzkpsjfM9e9PBg33IgRI8z69etNYmKi+fbbb01kZKSpWLGiOXHihDHGmH/961+matWqZu3atWbbtm0mIiLCREREuLjqonHmzBmzc+dOs3PnTiPJTJkyxezcudMcOXLEGGPM5MmTjZ+fn1m2bJnZvXu36dq1qwkLCzMXLlxwjNGxY0dz6623moSEBLNp0yZTq1Yt06dPH1e9pEJztbk5c+aMeeqpp0x8fLxJTEw033zzjWnSpImpVauWyczMdIxxM87NY489Znx9fc369evN8ePHHdv58+cdfa71M/T777+b+vXrm7vuusvs2rXLrFy50lSqVMnExcW54iUVmmvNzcGDB83EiRPNtm3bTGJiolm2bJmpXr26adOmjWOMm3VuRo0aZTZs2GASExPN7t27zahRo4zNZjNff/21Mebv+54x5upz44r3DEHGAnr16mUCAwONh4eHCQ4ONr169TIHDx50HL9w4YJ5/PHHTbly5UypUqXMfffdZ44fP+7CiovOunXrjKQcW0xMjDHmj1uwx4wZY/z9/Y2np6fp0KGD2bdvn9MYp06dMn369DFlypQxPj4+pn///ubMmTMueDWF62pzc/78eXPXXXeZSpUqGXd3dxMaGmoGDhzodPujMTfn3OQ2J5LM3LlzHX3y8zN0+PBh06lTJ+Pt7W0qVqxoRowYYS5dunSDX03hutbcJCUlmTZt2pjy5csbT09PU7NmTfP000+b9PR0p3Fuxrl55JFHTGhoqPHw8DCVKlUyHTp0cIQYY/6+7xljrj43rnjP2Iwx5vrP4wAAALgea2QAAIBlEWQAAIBlEWQAAIBlEWQAAIBlEWQAAIBlEWQAAIBlEWQAAIBlEWQAIBc2m01Lly51dRkAroEgAyBX7dq109ChQ11dhpPiWBMA1yLIAChSFy9edHUJAG5iBBkAOfTr108bNmzQ9OnTZbPZZLPZdPjwYWVlZWnAgAEKCwuTt7e3ateurenTp+d4bLdu3fTiiy8qKChItWvXliRt3rxZjRs3lpeXl5o1a6alS5fKZrM5vhlXkvbs2aNOnTqpTJky8vf310MPPaSTJ09etaYrPfvss7l+W3ejRo00ceJESdLWrVt15513qmLFivL19VXbtm21Y8eOPOdj/fr1stlsSktLc7Tt2rUrRw2bNm1S69at5e3trZCQED3xxBM6d+7ctaYbwF9AkAGQw/Tp0xUREaGBAwfq+PHjOn78uEJCQpSdna0qVapo8eLF2rt3r8aOHatnn31WixYtcnr8mjVrtG/fPq1evVpffPGFMjIy1KVLFzVo0EA7duzQ888/r5EjRzo9Ji0tTe3bt9ett96qbdu2aeXKlUpNTVXPnj2vWtOV+vbtq//97386dOiQo+2HH37Q7t279cADD0iSzpw5o5iYGG3atElbtmxRrVq1dPfdd+vMmTMFnrNDhw6pY8eOio6O1u7du/Xxxx9r06ZNGjx4cIHHBJAPf/FLMAHcpNq2bWuefPLJa/aLjY010dHRjv2YmBjj7+9v7Ha7o23WrFmmQoUK5sKFC462d955x0gyO3fuNMYY8/zzz5u77rrLaezk5GQjyfEN5vmtqVGjRmbixImO/bi4ONO8efM8+2dlZZmyZcua5cuXO9okmSVLlhhj/u+bxX/77TfH8Z07dxpJJjEx0RhjzIABA8yjjz7qNO5///tf4+bm5vS6ARQuzsgAuC4zZsxQ06ZNValSJZUpU0Zvv/22kpKSnPo0aNBAHh4ejv19+/apYcOG8vLycrTdfvvtTo/57rvvtG7dOpUpU8axhYeHS5LT2ZX86Nu3rxYsWCBJMsboo48+Ut++fR3HU1NTNXDgQNWqVUu+vr7y8fHR2bNnc7yO6/Hdd99p3rx5TvVHRUUpOztbiYmJBR4XwNWVdHUBAKxj4cKFeuqpp/Taa68pIiJCZcuW1auvvqqEhASnfqVLl77usc+ePasuXbro5ZdfznEsMDDwusbq06ePRo4cqR07dujChQtKTk5Wr169HMdjYmJ06tQpTZ8+XaGhofL09FRERESeC5Pd3P74P58xxtF26dKlHPUPGjRITzzxRI7HV61a9brqB5B/BBkAufLw8FBWVpZT27fffqs77rhDjz/+uKMtP2dLateurQ8//FB2u12enp6S/lhw+2dNmjTRp59+qmrVqqlkydz/acqtptxUqVJFbdu21fz583XhwgXdeeedqly5stPrmDlzpu6++25JUnJysmNRcW4qVaokSTp+/LjKlSsnSU6LlC/Xv3fvXtWsWfOa9QEoPFxaApCratWqKSEhQYcPH9bJkyeVnZ2tWrVqadu2bVq1apX279+vMWPG5AgkuXnggQeUnZ2tRx99VD/++KNWrVqlf//735L++OA5SYqNjdXp06fVp08fbd26VYcOHdKqVavUv39/R3jJraa89O3bVwsXLtTixYudLitJUq1atfTBBx/oxx9/VEJCgvr27Stvb+88x6pZs6ZCQkI0fvx4HThwQF9++aVee+01pz4jR47U5s2bNXjwYO3atUsHDhzQsmXLWOwLFDGCDIBcPfXUUypRooTq1q2rSpUqKSkpSYMGDVL37t3Vq1cvNW/eXKdOnXI6O5MXHx8fLV++XLt27VLjxo01evRojR07VpIc62aCgoL07bffKisrS3fddZcaNGigoUOHys/Pz3FpJ7ea8nL//ffr1KlTOn/+vLp16+Z0bM6cOfrtt9/UpEkTPfTQQ3riiSeczthcyd3dXR999JF++uknNWzYUC+//LJeeOEFpz4NGzbUhg0btH//frVu3Vq33nqrxo4dq6CgoGvOD4CCs5k/X/QFgBtk/vz56t+/v9LT0696NgQAroY1MgBuiPfff1/Vq1dXcHCwvvvuO40cOVI9e/YkxAD4SwgyAG6IlJQUjR07VikpKQoMDFSPHj304osvurosABbHpSUAAGBZLPYFAACWRZABAACWRZABAACWRZABAACWRZABAACWRZABAACWRZABAACWRZABAACWRZABAACW9f8AP5UQ/nS+EnUAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot a histogram to visualize the distribution of the \"target\" value\n",
    "plt.hist(data.frame['target'], rwidth=0.9)\n",
    "plt.title(\"Target value distribution\")\n",
    "plt.xlabel(\"target value\")\n",
    "plt.ylabel(\"#samples\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0964e70",
   "metadata": {},
   "source": [
    "### Prepare data for model training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73f1b21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into 70% training and 30% testing\n",
    "# Documentation: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "# Remember to set random_state to control for the randomness\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5172c982",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((309, 10), (133, 10))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check feature matrix\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a796cd9b",
   "metadata": {},
   "source": [
    "### Fit the linear regression model on the training set and evaluate model performance on the testing set \n",
    "> Documentation: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ca436c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.477"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg = LinearRegression() # check the documentation to understand the default parameters\n",
    "reg.fit(X_train, y_train)\n",
    "reg_score = reg.score(X_test, y_test)\n",
    "np.round(reg_score,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92373e7b",
   "metadata": {},
   "source": [
    "**Interpret model coefficients and intercept**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9f93ce14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  29.254, -261.706,  546.3  ,  388.398, -901.96 ,  506.763,\n",
       "        121.154,  288.035,  659.269,   41.377])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(reg.coef_,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f66d6311",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151.008"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(reg.intercept_,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a97b88",
   "metadata": {},
   "source": [
    "**Your task**: write down the linear regression model with the above coefficients and intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58175dd8",
   "metadata": {},
   "source": [
    "#29.254 * age +  -261.706 * sex +  546.3 * bmi + 388.398 * bp + -901.96 * s1 + 506.763 * s2 + 121.154 * s3 \n",
    "+ 288.035 * s4 +  659.269 * s5 + 41.377 * s6 + 151.008"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8baf4c48",
   "metadata": {},
   "source": [
    "**Your task**: explore other parameters/attributes/methods\n",
    "- fit_intercept\n",
    "- feature_names_in_, n_features_in_\n",
    "Write your exploration code and results below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "13242258",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.fit_intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faaa4334",
   "metadata": {},
   "source": [
    "indicated that the model has intercept term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "04ff41a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['age', 'sex', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.feature_names_in_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bee5e1d",
   "metadata": {},
   "source": [
    "This is an attribute. Include the feature names were used to train in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0c2b3496",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.n_features_in_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224e15a0",
   "metadata": {},
   "source": [
    "Number of features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc280db1",
   "metadata": {},
   "source": [
    "### Fit and evaluate a Ridge regression model (with the same train/test data)\n",
    "> Documentation: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "71a8d3a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.423"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rg_reg = Ridge()\n",
    "rg_reg.fit(X_train, y_train)\n",
    "rg_reg_score = rg_reg.score(X_test, y_test)\n",
    "np.round(rg_reg_score,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3c783ca4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  45.054,  -71.947,  280.716,  195.213,   -2.229,  -17.541,\n",
       "       -148.689,  120.467,  198.614,  106.935])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(rg_reg.coef_,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a4deaa4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151.867"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(rg_reg.intercept_,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b58d2f",
   "metadata": {},
   "source": [
    "### Your task: fit and evaluate a Lasso regression model (with the same train/test data)\n",
    "> Documentation: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1c416398",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.362"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit model\n",
    "lasso_reg = Lasso()\n",
    "\n",
    "lasso_reg.fit(X_train, y_train)\n",
    "lasso_reg_score = lasso_reg.score(X_test, y_test)\n",
    "np.round(lasso_reg_score,3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "71c90cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model performance\n",
    "\n",
    "# The lasso regression model has a lower performance score than the linear regression model.\n",
    "# 0.423 vs. 0.362\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9d8fc028",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.   ,  -0.   , 443.703,  51.601,   0.   ,   0.   ,  -0.   ,\n",
       "         0.   , 201.966,   0.   ])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(lasso_reg.coef_,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2c5dea51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpret model coefficients and intercept\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd2e329",
   "metadata": {},
   "source": [
    "We can see that many coefficents were reduced to 0, formed a sparse model (many zeros)\n",
    "Many features were penalized, thus also reduced the complexity of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "568f2212",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "152.166"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(lasso_reg.intercept_,3)\n",
    "\n",
    "# similar intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c950962e",
   "metadata": {},
   "source": [
    "### Your task: compare the linear/ridge/lasso regression models\n",
    "- write down your code to create and display the given dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "92e90c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hint: the following dataframe shows the expected way to organize and display the information\n",
    "# make sure to: \n",
    "# - round to 3 digits after the decimal point\n",
    "# - rename the column names \n",
    "# - include intercept and score in the last two rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "66ccb4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cannot load the table name from the lastest updated version of M2.2 file\n",
    "# So I had to skip the table part of the question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d7e638",
   "metadata": {},
   "source": [
    "**Your observations and thoughts of comparing the three models**\n",
    "- hint: connect this with what we discussed in the lectures, e.g.\n",
    "    - how does regularization affect coefficients and model performance \n",
    "    - what is the difference between ridge (L2 penalty) and Lasso (L1 penalty) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46984af",
   "metadata": {},
   "source": [
    "Lasso (L1 penalty) \n",
    "- Lasso regression has reduced several coefficients to zero, which is a hallmark of L1 regularization. This method is useful for feature selection as it tends to produce sparse models, effectively ignoring non-influential features.\n",
    "- In this case, lasso has determined that 'age', 'sex', 's1', 's2', 's3', and 's6' do not contribute significantly to the model and has removed them by setting their coefficients to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab08ff52",
   "metadata": {},
   "source": [
    "Ridge (L2 penalty)\n",
    "- Ridge regression has not reduced any coefficients to zero but has shrunk them closer to zero compared to the linear regression model. This is typical of L2 regularization which penalizes the square of the coefficients and tends to distribute the coefficient values more evenly among the features.\n",
    "- Here, 's1' and 's2' have small magnitude coefficients in ridge regression, suggesting they are less important than the other features but still contribute information to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648bc8be",
   "metadata": {},
   "source": [
    "Overall Performance:\n",
    "- The standard linear regression has the highest score, suggesting that without regularization, it fits the training data best among the three.\n",
    "- Ridge regression has a slightly lower score than linear regression, indicating a small trade-off in performance due to regularization, which can benefit the model's performance on unseen data by reducing overfitting.\n",
    "- Lasso regression has the lowest score, which could mean that while it provides a more interpretable model by selecting features, it may be too simplistic for the underlying complexity of the data, thus not capturing all the necessary relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cca80a6",
   "metadata": {},
   "source": [
    "## Polynomial regression\n",
    "- Use the diabetes data with the same train and test set to fit several **Polynomial regression** models \n",
    "- documentation: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61aaa8fc",
   "metadata": {},
   "source": [
    "### Fit a polynomail regression model with degree=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5c660e65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(309, 66)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate the new polynomial feature matrix for the training data \n",
    "# \"fit_transform\" on the original training data\n",
    "\n",
    "poly2 = PolynomialFeatures(2)\n",
    "X_train_poly2 = poly2.fit_transform(X_train)\n",
    "X_train_poly2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "da88debb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(133, 66)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate the new polynomial feature matrix for the testing data \n",
    "# only do \"transform\" on the original testing feature matrix\n",
    "# why do we do \"fit_transform\" on training data but only do \"transform\" on testing data?\n",
    "\n",
    "X_test_poly2 = poly2.transform(X_test)\n",
    "X_test_poly2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "de488def",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.413"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit a regression model with the newly generated polynomial feature matrix \n",
    "# evaluate model performance \n",
    "\n",
    "poly2_reg = LinearRegression()\n",
    "poly2_reg.fit(X_train_poly2, y_train)\n",
    "poly2_score = poly2_reg.score(X_test_poly2, y_test)\n",
    "np.round(poly2_score,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72079596",
   "metadata": {},
   "source": [
    "### Your task: fit a polynomail regression model with degree=1\n",
    "- Follow the previous steps of fitting a polynomial regression model with degree=2 to **fit a new model with degree=1** (name it as $poly1\\_reg$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e53f151c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(309, 11)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate the new polynomial feature matrix for the training data \n",
    "# \"fit_transform\" on the original training data\n",
    "\n",
    "poly1 = PolynomialFeatures(1)\n",
    "X_train_poly1 = poly1.fit_transform(X_train)\n",
    "X_train_poly1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "794b0562",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(133, 11)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate the new polynomial feature matrix for the testing data \n",
    "# only do \"transform\" on the original testing feature matrix\n",
    "# why do we do \"fit_transform\" on training data but only do \"transform\" on testing data?\n",
    "\n",
    "X_test_poly1 = poly1.transform(X_test)\n",
    "X_test_poly1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3f9e86ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.477"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit a regression model with the newly generated polynomial feature matrix \n",
    "# evaluate model performance \n",
    "\n",
    "poly1_reg = LinearRegression()\n",
    "poly1_reg.fit(X_train_poly1, y_train)\n",
    "poly1_score = poly1_reg.score(X_test_poly1, y_test)\n",
    "np.round(poly1_score,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2b9642",
   "metadata": {},
   "source": [
    "### Your task: fit a polynomail regression model with degree=3\n",
    "- Follow the previous steps of fitting a polynomial regression model with degree=2 to **fit a new model with degree=3** (name it as $poly3\\_reg$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8760be74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(309, 286)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate the new polynomial feature matrix for the training data \n",
    "# \"fit_transform\" on the original training data\n",
    "\n",
    "poly3 = PolynomialFeatures(3)\n",
    "X_train_poly3 = poly3.fit_transform(X_train)\n",
    "X_train_poly3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8c0b9490",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(133, 286)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate the new polynomial feature matrix for the testing data \n",
    "# only do \"transform\" on the original testing feature matrix\n",
    "# why do we do \"fit_transform\" on training data but only do \"transform\" on testing data?\n",
    "\n",
    "X_test_poly3 = poly3.transform(X_test)\n",
    "X_test_poly3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d29a3915",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-92.443"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit a regression model with the newly generated polynomial feature matrix \n",
    "# evaluate model performance \n",
    "\n",
    "poly3_reg = LinearRegression()\n",
    "poly3_reg.fit(X_train_poly3, y_train)\n",
    "poly3_score = poly3_reg.score(X_test_poly3, y_test)\n",
    "np.round(poly3_score,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc159ab",
   "metadata": {},
   "source": [
    "### Your task: compare the polynomial regression models with degree=1/2/3 and the original linear regression model\n",
    "- please write code to create and display the given data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d7e22589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Model  Degree   Score  Rounded_Score\n",
      "0  linear       1   0.477          0.477\n",
      "1   poly1       1   0.477          0.477\n",
      "2   poly2       2   0.413          0.413\n",
      "3   poly3       3 -92.443        -92.443\n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "    'Model': ['linear', 'poly1', 'poly2', 'poly3'],\n",
    "    'Degree': [1, 1, 2, 3],\n",
    "    'Score': [0.477, 0.477, 0.413, -92.443]\n",
    "}\n",
    "\n",
    "# Create the DataFrame\n",
    "df_comparison = pd.DataFrame(data)\n",
    "\n",
    "# Add a column with the rounded scores\n",
    "df_comparison['Rounded_Score'] = np.round(df_comparison['Score'], 3)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df_comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4b32b9",
   "metadata": {},
   "source": [
    "- Polynomial Regression (degree 1): 0.477\n",
    "- Polynomial Regression (degree 2): 0.413\n",
    "- Polynomial Regression (degree 3): -92.443\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051b17b2",
   "metadata": {},
   "source": [
    "### Your task: observations and thoughts of comparing the above four models\n",
    "- hint: connect this with overfitting/underfitting we discussed in class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167f6166",
   "metadata": {},
   "source": [
    "The polynomial regression with degree 2 has a lower score than the degree 1 models, indicating that adding squared terms does not improve the model performance on the test dataset.\n",
    "\n",
    "The degree 3 is too complex and overfitting, it has a significantly negative score, which is highly unusual and indicates that the model is performing extremely poorly. Suggesting that the model learned the complex relationship from the learning data but it is not applicable for further prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e005bd8",
   "metadata": {},
   "source": [
    "### Your task: interpret the model performance wrt the task itself\n",
    "- how does each feature relate with diabetes\n",
    "- which factors contribute positively/negatively/most/least to diabetes\n",
    "- does these statistical correlations make sense from biological perspective? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "56641a54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.37163693e+16, -6.90773180e+04, -1.83330332e+05, -5.61627024e+05,\n",
       "       -6.28216930e+04, -4.67065617e+06,  3.70512052e+06,  8.70542637e+05,\n",
       "       -4.01738976e+05,  6.19073642e+05,  1.85548896e+05, -1.30286310e+04,\n",
       "        9.24641000e+02,  1.87770080e+04,  2.44157500e+03, -1.64090649e+07,\n",
       "        1.44196155e+07,  6.14174188e+06,  1.35325840e+04,  5.38413600e+06,\n",
       "        1.97808000e+03, -1.10653100e+03, -1.10515210e+04, -1.04901140e+04,\n",
       "       -4.22523639e+07,  3.71646085e+07,  1.57686186e+07, -5.79698770e+04,\n",
       "        1.39352857e+07,  4.50059200e+03, -1.34321600e+03,  9.40348900e+03,\n",
       "       -1.29503055e+08,  1.13788892e+08,  4.84212100e+07,  2.29366200e+04,\n",
       "        4.25940619e+07, -4.01307000e+03, -5.20008700e+03, -1.45534352e+07,\n",
       "        1.27500112e+07,  5.45959558e+06,  1.33390400e+04,  4.78982902e+06,\n",
       "        2.24795180e+04, -6.62193341e+08,  1.07207451e+09,  2.92982214e+08,\n",
       "       -9.32623857e+07,  2.30381680e+08,  4.27248028e+07, -4.30728969e+08,\n",
       "       -2.23162584e+08,  8.19028462e+07, -1.72215337e+08, -3.75421030e+07,\n",
       "       -1.69860410e+07,  3.49024302e+07, -1.97300885e+07, -1.59430115e+07,\n",
       "        7.13973810e+04,  3.05609408e+07,  4.91111530e+04, -9.67265725e+05,\n",
       "       -1.40570143e+07, -1.85135680e+04, -1.19020121e+05, -1.35459282e+05,\n",
       "       -2.22195060e+04,  4.64347612e+05, -1.28791466e+06,  1.60704911e+06,\n",
       "       -4.25997595e+05, -1.09700917e+06,  1.35242100e+04,  1.37373787e+05,\n",
       "       -1.50521000e+02,  1.33919449e+05, -3.44101533e+05, -1.29980834e+05,\n",
       "        6.14455499e+05, -7.07750100e+04, -2.34701480e+05,  9.54394710e+04,\n",
       "        1.05980318e+05, -9.91563580e+04,  3.09995980e+05,  1.48979144e+06,\n",
       "       -1.99714564e+06,  1.10335830e+04,  4.78769140e+05, -6.18190588e+05,\n",
       "        2.11358095e+05,  5.68924910e+04, -3.91575004e+06,  2.84686789e+06,\n",
       "        2.34945185e+06,  1.33879804e+06,  3.15925035e+05, -6.49501874e+05,\n",
       "       -2.48920198e+07,  4.88103222e+07,  1.47706882e+07, -6.99086061e+06,\n",
       "        8.02207728e+07, -1.36583381e+06, -2.49853991e+07, -1.24624550e+07,\n",
       "        1.02294490e+07, -7.20719372e+07,  1.28552327e+06, -2.58238492e+06,\n",
       "       -2.94355518e+05, -2.78027804e+07,  5.86548760e+04, -2.95142484e+06,\n",
       "        3.01394574e+06, -8.36217442e+05,  5.10233762e+06,  1.09932195e+06,\n",
       "        4.10617610e+04, -4.21404000e+02, -1.33744600e+03, -2.05512000e+02,\n",
       "       -2.65707360e+05,  2.32800564e+05,  9.71881990e+04, -1.25898100e+03,\n",
       "        8.55486720e+04,  4.47138000e+02,  2.50071603e+05, -4.04498100e+04,\n",
       "       -5.31541497e+06,  4.14738593e+06,  2.81673233e+06,  1.59400604e+06,\n",
       "        1.60879542e+06,  1.56634670e+04, -3.55800292e+05, -3.60736892e+06,\n",
       "        3.56812045e+06,  3.74873008e+05, -1.34909910e+06,  1.71459081e+06,\n",
       "        2.55185904e+05, -3.67308655e+07,  6.87210314e+07,  2.06973783e+07,\n",
       "       -8.65078254e+06,  1.82401524e+08, -9.82870250e+05, -3.21388254e+07,\n",
       "       -2.02361816e+07,  7.54258184e+06, -1.61737839e+08,  6.10632358e+05,\n",
       "       -2.20356212e+06,  4.46687062e+06, -6.65708496e+07,  6.27010951e+05,\n",
       "        3.90575446e+05,  2.51351068e+06,  1.78950227e+05,  1.74768746e+07,\n",
       "        3.27579026e+05, -1.17790678e+05, -5.94692200e+04, -1.70323282e+05,\n",
       "       -5.57858671e+06,  4.92932925e+06,  2.16117364e+06, -3.71305523e+05,\n",
       "        2.52243554e+06, -1.85448106e+05, -2.62707320e+04,  1.47899204e+06,\n",
       "       -9.03809664e+05, -9.13580478e+05, -4.26704337e+05, -9.91427193e+05,\n",
       "        1.71740007e+05, -2.16547016e+08,  3.79725023e+08,  1.61110957e+08,\n",
       "       -5.51713637e+06,  6.17118610e+08,  6.91675446e+06, -1.65535749e+08,\n",
       "       -1.43241046e+08,  1.90887825e+06, -5.41937455e+08, -4.93893444e+06,\n",
       "       -2.83545546e+07,  6.60952844e+06, -2.29911807e+08, -4.38011651e+06,\n",
       "        3.02612626e+06,  2.25964768e+06, -2.27723275e+06,  4.70277152e+07,\n",
       "       -2.56075997e+06,  2.47395197e+05,  6.62349990e+04, -2.23920723e+06,\n",
       "        1.83843853e+06,  7.53600303e+05,  1.68737523e+05,  8.91199483e+05,\n",
       "        5.03972900e+03, -5.35734580e+07,  8.56606115e+07,  4.62096630e+07,\n",
       "        3.64758368e+06,  9.10610885e+07,  3.60233169e+06, -3.38868611e+07,\n",
       "       -3.70176818e+07, -3.10278341e+06, -7.70891961e+07, -3.36868593e+06,\n",
       "       -1.05140582e+07, -2.50417766e+06, -3.56482007e+07, -6.95055977e+05,\n",
       "       -5.85321980e+04, -1.52052008e+06,  7.40947947e+05,  1.48190282e+06,\n",
       "       -4.57728900e+04, -5.06051777e+05, -5.31147975e+08,  1.20516978e+09,\n",
       "        3.63880073e+08, -7.15803409e+07,  2.56499312e+09,  5.23250423e+07,\n",
       "       -8.97610054e+08, -4.71254430e+08,  1.50949316e+08, -4.04381631e+09,\n",
       "       -9.45378308e+07, -4.86258076e+07,  3.06447519e+07, -1.03054082e+09,\n",
       "       -3.55771799e+07, -1.56025733e+07,  3.83752264e+08,  7.75105592e+06,\n",
       "        3.95200139e+08, -1.83714007e+08, -7.80466579e+06,  2.18589191e+08,\n",
       "        1.37848536e+08, -7.65416530e+07,  1.57583369e+09,  4.26937634e+07,\n",
       "        5.10713080e+06, -4.03244650e+07,  7.23474251e+08,  3.26968624e+07,\n",
       "        1.19876845e+07, -3.46346838e+08, -6.60861802e+06, -3.18253935e+08,\n",
       "        1.61966391e+08,  6.71016160e+06, -4.11540752e+06,  1.34074631e+06,\n",
       "        2.73127174e+07,  5.98552133e+06,  8.55913188e+06, -1.34270468e+08,\n",
       "       -2.85268035e+06, -6.24907015e+07,  6.77614417e+07,  3.41618072e+06,\n",
       "        5.46133344e+05,  5.88112343e+06, -2.24925857e+05,  4.41347369e+07,\n",
       "       -1.83955469e+06,  7.40224562e+05, -1.93221863e+07, -2.02472811e+07,\n",
       "        2.32661957e+06,  2.26506000e+03])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(poly3_reg.coef_,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "72949570",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00000000e+00,  1.14383000e+02, -3.28441000e+02,  4.15272000e+02,\n",
       "        4.42233000e+02, -1.14471315e+05,  1.00519161e+05,  4.24092550e+04,\n",
       "       -2.08250000e+01,  3.82227860e+04, -3.34800000e+00,  2.81753100e+03,\n",
       "        6.44434000e+02, -2.19092100e+03,  1.73861300e+03,  1.28889500e+03,\n",
       "       -7.98148900e+03,  2.77767300e+03,  8.11251200e+03,  1.39044000e+02,\n",
       "        2.93792000e+02, -1.98300000e+00, -4.67337000e+02,  1.51863600e+03,\n",
       "       -6.37036900e+03,  8.43408800e+03,  5.00109000e+02, -5.51912100e+03,\n",
       "        4.76977700e+03,  3.88107000e+02,  5.43261000e+02,  4.01004100e+03,\n",
       "       -9.44177700e+03,  7.29725500e+03,  2.98515100e+03, -4.30240000e+02,\n",
       "        3.69275600e+03,  1.88231300e+03, -2.55802000e+02,  2.46438660e+04,\n",
       "       -1.71935320e+04, -1.10066760e+04, -3.70916400e+03, -9.57339000e+03,\n",
       "       -4.55804400e+03, -4.22326150e+04,  1.00197282e+05,  5.22287870e+04,\n",
       "       -6.89849000e+03,  4.01069079e+05, -5.69639900e+03, -5.12237350e+04,\n",
       "       -6.14823720e+04, -6.70404400e+03, -3.62258923e+05,  3.89037600e+03,\n",
       "       -1.17124830e+04,  7.13769300e+03, -1.53429483e+05,  6.26408700e+03,\n",
       "        8.37790700e+03,  1.96683600e+03,  3.90937800e+03,  7.16133250e+04,\n",
       "        5.85017700e+03,  1.11252100e+03])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(poly2_reg.coef_,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b3064874",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  29.254, -261.706,  546.3  ,  388.398, -901.96 ,  506.763,\n",
       "        121.154,  288.035,  659.269,   41.377])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(reg.coef_,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "feb718f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['age', 'sex', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.feature_names_in_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75861d2a",
   "metadata": {},
   "source": [
    " the model's coefficients indicate that BMI and blood pressure have the most substantial positive associations with diabetes progression, affirming well-known clinical correlations; higher BMI and blood pressure are recognized risk factors for the exacerbation of diabetes. Conversely, the negative coefficient for sex suggests a gender disparity in disease progression, hinting at possible biological differences or varying impacts of treatment between genders. Serum measurements present a mixed picture; some positively correlate with disease progression, perhaps indicating metabolic imbalances linked to diabetes, while others, like s1, show a negative relationship, suggesting a potential protective effect. These statistical insights, In the context of this dataset, while some factors like bmi and bp have well-established relationships with diabetes from a biological standpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef82f53",
   "metadata": {},
   "source": [
    "## Linear models for classification: LogisticRegression \n",
    "In this section, we will work on a banknote authentication dataset:\n",
    "- Original data source: https://archive.ics.uci.edu/ml/datasets/banknote+authentication <br>\n",
    "\n",
    "This dataset contains n = 1372 images of genuine and forged banknote-like specimens. Each image is represented by four features extracted from Wavelet Transform tool: \n",
    "    1. variance (continuous) \n",
    "    2. skewness (continuous)\n",
    "    3. curtosis (continuous)\n",
    "    4. entropy of image (continuous)\n",
    "\n",
    "And each image has a binary label of 0/1 indicating whether the banknote is forged or genuine.\n",
    "\n",
    "We will fit several logistic regression models with different parameter settings to analyze this dataset: \n",
    "The steps include:\n",
    "1. Basic data exploration:\n",
    "    > what does the data look like (#samples, #features) <br>\n",
    "    > the feature matrix and description of each feature <br>\n",
    "    > the target values <br>\n",
    "    \n",
    "2. Prepare data for model training and testing <br>\n",
    "\n",
    "3. Fit different logistic regression models (vary by parameter settings) on the training set and evaluate model performance on the testing set <br>\n",
    "\n",
    "4. Compare and understand model performance through interpreting coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "574468b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cf817f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# please change to your file path\n",
    "df_data = pickle.load(open('/Users/adhdtreamentii/Desktop/UChicago 2024/UChicago Winter 24/MACSS-30100/Linear models/banknote_authentication_dataframe (1).pickle','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a442ec9",
   "metadata": {},
   "source": [
    "### Basic dataset exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a876742b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variance</th>\n",
       "      <th>skewness</th>\n",
       "      <th>curtosis</th>\n",
       "      <th>entropy</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.62160</td>\n",
       "      <td>8.66610</td>\n",
       "      <td>-2.8073</td>\n",
       "      <td>-0.44699</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.54590</td>\n",
       "      <td>8.16740</td>\n",
       "      <td>-2.4586</td>\n",
       "      <td>-1.46210</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.86600</td>\n",
       "      <td>-2.63830</td>\n",
       "      <td>1.9242</td>\n",
       "      <td>0.10645</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.45660</td>\n",
       "      <td>9.52280</td>\n",
       "      <td>-4.0112</td>\n",
       "      <td>-3.59440</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.32924</td>\n",
       "      <td>-4.45520</td>\n",
       "      <td>4.5718</td>\n",
       "      <td>-0.98880</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1367</th>\n",
       "      <td>0.40614</td>\n",
       "      <td>1.34920</td>\n",
       "      <td>-1.4501</td>\n",
       "      <td>-0.55949</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1368</th>\n",
       "      <td>-1.38870</td>\n",
       "      <td>-4.87730</td>\n",
       "      <td>6.4774</td>\n",
       "      <td>0.34179</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1369</th>\n",
       "      <td>-3.75030</td>\n",
       "      <td>-13.45860</td>\n",
       "      <td>17.5932</td>\n",
       "      <td>-2.77710</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1370</th>\n",
       "      <td>-3.56370</td>\n",
       "      <td>-8.38270</td>\n",
       "      <td>12.3930</td>\n",
       "      <td>-1.28230</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1371</th>\n",
       "      <td>-2.54190</td>\n",
       "      <td>-0.65804</td>\n",
       "      <td>2.6842</td>\n",
       "      <td>1.19520</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1372 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      variance  skewness  curtosis  entropy  class\n",
       "0      3.62160   8.66610   -2.8073 -0.44699    0.0\n",
       "1      4.54590   8.16740   -2.4586 -1.46210    0.0\n",
       "2      3.86600  -2.63830    1.9242  0.10645    0.0\n",
       "3      3.45660   9.52280   -4.0112 -3.59440    0.0\n",
       "4      0.32924  -4.45520    4.5718 -0.98880    0.0\n",
       "...        ...       ...       ...      ...    ...\n",
       "1367   0.40614   1.34920   -1.4501 -0.55949    1.0\n",
       "1368  -1.38870  -4.87730    6.4774  0.34179    1.0\n",
       "1369  -3.75030 -13.45860   17.5932 -2.77710    1.0\n",
       "1370  -3.56370  -8.38270   12.3930 -1.28230    1.0\n",
       "1371  -2.54190  -0.65804    2.6842  1.19520    1.0\n",
       "\n",
       "[1372 rows x 5 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display the dataset\n",
    "df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "222a148c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variance</th>\n",
       "      <th>skewness</th>\n",
       "      <th>curtosis</th>\n",
       "      <th>entropy</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1372.000000</td>\n",
       "      <td>1372.000000</td>\n",
       "      <td>1372.000000</td>\n",
       "      <td>1372.000000</td>\n",
       "      <td>1372.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.433735</td>\n",
       "      <td>1.922353</td>\n",
       "      <td>1.397627</td>\n",
       "      <td>-1.191657</td>\n",
       "      <td>0.444606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.842763</td>\n",
       "      <td>5.869047</td>\n",
       "      <td>4.310030</td>\n",
       "      <td>2.101013</td>\n",
       "      <td>0.497103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-7.042100</td>\n",
       "      <td>-13.773100</td>\n",
       "      <td>-5.286100</td>\n",
       "      <td>-8.548200</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-1.773000</td>\n",
       "      <td>-1.708200</td>\n",
       "      <td>-1.574975</td>\n",
       "      <td>-2.413450</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.496180</td>\n",
       "      <td>2.319650</td>\n",
       "      <td>0.616630</td>\n",
       "      <td>-0.586650</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.821475</td>\n",
       "      <td>6.814625</td>\n",
       "      <td>3.179250</td>\n",
       "      <td>0.394810</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>6.824800</td>\n",
       "      <td>12.951600</td>\n",
       "      <td>17.927400</td>\n",
       "      <td>2.449500</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          variance     skewness     curtosis      entropy        class\n",
       "count  1372.000000  1372.000000  1372.000000  1372.000000  1372.000000\n",
       "mean      0.433735     1.922353     1.397627    -1.191657     0.444606\n",
       "std       2.842763     5.869047     4.310030     2.101013     0.497103\n",
       "min      -7.042100   -13.773100    -5.286100    -8.548200     0.000000\n",
       "25%      -1.773000    -1.708200    -1.574975    -2.413450     0.000000\n",
       "50%       0.496180     2.319650     0.616630    -0.586650     0.000000\n",
       "75%       2.821475     6.814625     3.179250     0.394810     1.000000\n",
       "max       6.824800    12.951600    17.927400     2.449500     1.000000"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# describe the data \n",
    "df_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcbafff",
   "metadata": {},
   "source": [
    "### Prepara data for model training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1fbc714a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1372, 4), (1372,), Counter({0.0: 762, 1.0: 610}))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = ['variance','skewness','curtosis','entropy']\n",
    "\n",
    "# Construct feature matrix from the data frame\n",
    "X_data = df_data[feature_names]\n",
    "y_data = df_data['class']\n",
    "X_data.shape, y_data.shape, Counter(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "98dc3fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into 70% training and 30% testing using train_test_split()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3bb9b2",
   "metadata": {},
   "source": [
    "### Fit LogisticRegression models with different parameter settings\n",
    "- L1 VS L2 penalty\n",
    "- C values (inverse of regularization strength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2e206acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(random_state=0, solver='liblinear', penalty='l1', C=1.0).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cb8b57ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9878640776699029"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0baa21e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0.])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(X_test[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2525662f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.   , 0.   ],\n",
       "       [0.982, 0.018],\n",
       "       [0.996, 0.004]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(clf.predict_proba(X_test[:3]),3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8ff19c",
   "metadata": {},
   "source": [
    "**Your task**: explore at least one different set of parameters to re-fit the model: solver, penalty, C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "598e50e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9878640776699029\n",
      "[0. 0. 0.]\n",
      "[[1.    0.   ]\n",
      " [0.963 0.037]\n",
      " [0.987 0.013]]\n"
     ]
    }
   ],
   "source": [
    "# L2 penalty shows a very similar performance to L1 penalty\n",
    "\n",
    "clf2 = LogisticRegression(random_state=0, solver='liblinear', penalty='l2', C=1.0).fit(X_train, y_train)\n",
    "\n",
    "print(clf2.score(X_test, y_test))\n",
    "\n",
    "print(clf2.predict(X_test[:3]))\n",
    "\n",
    "print(np.round(clf2.predict_proba(X_test[:3]),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f8d4ddc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9174757281553398\n",
      "[0. 0. 0.]\n",
      "[[0.945 0.055]\n",
      " [0.75  0.25 ]\n",
      " [0.792 0.208]]\n"
     ]
    }
   ],
   "source": [
    "clf3 = LogisticRegression(random_state=0, solver='liblinear', penalty='l1', C=0.01).fit(X_train, y_train)\n",
    "\n",
    "print(clf3.score(X_test, y_test))\n",
    "\n",
    "print(clf3.predict(X_test[:3]))\n",
    "\n",
    "print(np.round(clf3.predict_proba(X_test[:3]),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c5d0d1",
   "metadata": {},
   "source": [
    "### Compare model performance with different c values and different penalties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9160ab01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compare_c(X_train, y_train, X_test, y_test, p):\n",
    "    \"\"\"\n",
    "    X_train/test: 2D feature matrix of training/testing data\n",
    "    y_train/test: 1D array of training/testing labels\n",
    "    p: the penalty parameter setting in LogisticRegression\n",
    "    \n",
    "    return: \n",
    "        a list of classifiers fitted with different c values\n",
    "        a dataframe that is shown in the running example below\n",
    "    \"\"\"\n",
    "     \n",
    "    # set the model parameter c to different values and train the model \n",
    "    # for c in [0.001, 0.01, 0.1, 1, 10, 100]:\n",
    "    #    fit a LogisticRegression model with: the current c value, the given penalty p, set random_state=42, max_iter=1000, solver='liblinear', and use default setting for other parameters\n",
    "    #    test and record the model performance \n",
    "    #    get the statistical information about the model coefficients: \n",
    "    #        min: minimum coefficient\n",
    "    #        max: minimum coefficient\n",
    "    #        mean(abs(coef)): average over the absolute coefficient values\n",
    "    #        n_zero: number of coefficients equal to zero \n",
    "    \n",
    "    ### Your code starts from here \n",
    "    classifiers = []\n",
    "    metrics_df = pd.DataFrame()\n",
    "    \n",
    "\n",
    "    # L2 penalty shows a very similar performance to L1 penalty\n",
    "    for c in [0.001, 0.01, 0.1, 1, 10, 100]:\n",
    "        clf = LogisticRegression(random_state=42, max_iter=1000, solver='liblinear', penalty=p, C=c)\n",
    "        clf.fit(X_train, y_train)\n",
    "        score = clf.score(X_test, y_test)\n",
    "        coef_stats = {\n",
    "            'min': np.min(clf.coef_),\n",
    "            'max': np.max(clf.coef_),\n",
    "            'mean_abs_coef': np.mean(np.abs(clf.coef_)),\n",
    "            'n_zero': np.sum(clf.coef_ == 0)\n",
    "        }\n",
    "        classifiers.append(clf)\n",
    "        metrics_df = metrics_df.append(coef_stats, ignore_index=True)\n",
    "\n",
    "    return classifiers, metrics_df\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d7fb1b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sm/t07r9cb5347gwrxg17tf23th0000gn/T/ipykernel_12475/1271663665.py:41: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  metrics_df = metrics_df.append(coef_stats, ignore_index=True)\n",
      "/var/folders/sm/t07r9cb5347gwrxg17tf23th0000gn/T/ipykernel_12475/1271663665.py:41: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  metrics_df = metrics_df.append(coef_stats, ignore_index=True)\n",
      "/var/folders/sm/t07r9cb5347gwrxg17tf23th0000gn/T/ipykernel_12475/1271663665.py:41: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  metrics_df = metrics_df.append(coef_stats, ignore_index=True)\n",
      "/var/folders/sm/t07r9cb5347gwrxg17tf23th0000gn/T/ipykernel_12475/1271663665.py:41: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  metrics_df = metrics_df.append(coef_stats, ignore_index=True)\n",
      "/var/folders/sm/t07r9cb5347gwrxg17tf23th0000gn/T/ipykernel_12475/1271663665.py:41: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  metrics_df = metrics_df.append(coef_stats, ignore_index=True)\n",
      "/var/folders/sm/t07r9cb5347gwrxg17tf23th0000gn/T/ipykernel_12475/1271663665.py:41: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  metrics_df = metrics_df.append(coef_stats, ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>mean_abs_coef</th>\n",
       "      <th>n_zero</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.357242</td>\n",
       "      <td>-0.074218</td>\n",
       "      <td>0.189712</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.860815</td>\n",
       "      <td>-0.172662</td>\n",
       "      <td>0.485241</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.580581</td>\n",
       "      <td>-0.162763</td>\n",
       "      <td>0.915028</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-2.834711</td>\n",
       "      <td>-0.166099</td>\n",
       "      <td>1.645101</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-5.171020</td>\n",
       "      <td>-0.289579</td>\n",
       "      <td>2.936961</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-7.647564</td>\n",
       "      <td>-0.437990</td>\n",
       "      <td>4.297064</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        min       max  mean_abs_coef  n_zero\n",
       "0 -0.357242 -0.074218       0.189712     0.0\n",
       "1 -0.860815 -0.172662       0.485241     0.0\n",
       "2 -1.580581 -0.162763       0.915028     0.0\n",
       "3 -2.834711 -0.166099       1.645101     0.0\n",
       "4 -5.171020 -0.289579       2.936961     0.0\n",
       "5 -7.647564 -0.437990       4.297064     0.0"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# running example\n",
    "l2_clfs, c_effect_l2 = compare_c(X_train, y_train, X_test, y_test, p='l2')\n",
    "c_effect_l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7bd278a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sm/t07r9cb5347gwrxg17tf23th0000gn/T/ipykernel_12475/1271663665.py:41: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  metrics_df = metrics_df.append(coef_stats, ignore_index=True)\n",
      "/var/folders/sm/t07r9cb5347gwrxg17tf23th0000gn/T/ipykernel_12475/1271663665.py:41: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  metrics_df = metrics_df.append(coef_stats, ignore_index=True)\n",
      "/var/folders/sm/t07r9cb5347gwrxg17tf23th0000gn/T/ipykernel_12475/1271663665.py:41: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  metrics_df = metrics_df.append(coef_stats, ignore_index=True)\n",
      "/var/folders/sm/t07r9cb5347gwrxg17tf23th0000gn/T/ipykernel_12475/1271663665.py:41: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  metrics_df = metrics_df.append(coef_stats, ignore_index=True)\n",
      "/var/folders/sm/t07r9cb5347gwrxg17tf23th0000gn/T/ipykernel_12475/1271663665.py:41: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  metrics_df = metrics_df.append(coef_stats, ignore_index=True)\n",
      "/var/folders/sm/t07r9cb5347gwrxg17tf23th0000gn/T/ipykernel_12475/1271663665.py:41: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  metrics_df = metrics_df.append(coef_stats, ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>mean_abs_coef</th>\n",
       "      <th>n_zero</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.041929</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010482</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.807180</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.327752</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.750236</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.935861</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-3.838485</td>\n",
       "      <td>-0.132006</td>\n",
       "      <td>2.163931</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-7.109730</td>\n",
       "      <td>-0.388707</td>\n",
       "      <td>3.992891</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-8.196342</td>\n",
       "      <td>-0.463991</td>\n",
       "      <td>4.595852</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        min       max  mean_abs_coef  n_zero\n",
       "0 -0.041929  0.000000       0.010482     3.0\n",
       "1 -0.807180  0.000000       0.327752     1.0\n",
       "2 -1.750236  0.000000       0.935861     1.0\n",
       "3 -3.838485 -0.132006       2.163931     0.0\n",
       "4 -7.109730 -0.388707       3.992891     0.0\n",
       "5 -8.196342 -0.463991       4.595852     0.0"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# running example\n",
    "l1_clfs, c_effect_l1 = compare_c(X_train, y_train, X_test, y_test, p='l1')\n",
    "c_effect_l1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934a8855",
   "metadata": {},
   "source": [
    "**Your thoughts and observations:** \n",
    "  - explain model performance from the perspective of under-fitting VS over-fitting\n",
    "  - compare the two tables and indicate the difference between L1 and L2 penalty\n",
    "  - how does c affect coefficients and model performance in each table "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa39510",
   "metadata": {},
   "source": [
    "For the L1 penalty, lower C values (stronger regularization) lead to sparser models, as indicated by a higher number of zero coefficients, particularly at C=0.001 and C=0.01. This sparsity diminishes as C increases, reflecting the reduction in regularization strength. The L1 model with C=0.001 notably underperforms, suggesting underfitting due to excessive regularization. However, as C increases, the model's performance improves significantly, stabilizing at a high accuracy level from C=0.1 onwards, indicating an optimal balance between model complexity and regularization.\n",
    "\n",
    "The L2 penalty models exhibit a consistent number of zero coefficients across all C values, as L2 does not inherently promote coefficient sparsity. Instead, the focus is on shrinking the coefficients evenly, preventing any single feature from disproportionately influencing the model. As with L1, the L2 models show improved performance with increasing C values, with a notable jump in accuracy between C=0.01 and C=0.1, and then plateauing. This pattern suggests that while both L1 and L2 models benefit from some degree of regularization to prevent overfitting, overly strong regularization (very small C values) is detrimental, leading to underfitting. Both penalties achieve high accuracy at higher C values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd39d68",
   "metadata": {},
   "source": [
    "### Interpret the model performance wrt the banknote authentication task\n",
    "- how does each feature relate with the identification of genuine and forged banknote\n",
    "- does these statistical correlations make sense from the perspective of image recognition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0440979",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.852, -2.117, -2.585, -0.133]])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(clf.coef_,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d548c94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['variance', 'skewness', 'curtosis', 'entropy'], dtype=object)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.feature_names_in_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41f9d83",
   "metadata": {},
   "source": [
    " these correlations make sense\n",
    "\n",
    " variance (-3.852), skewness (-2.117), curtosis (-2.585), and entropy (-0.133). These coefficients suggest that all four features negatively correlate with the likelihood of a banknote being genuine. Specifically, a decrease in variance, skewness, and curtosis increases the probability of a banknote being authentic. Variance, with the highest negative coefficient, is particularly influential, indicating that lower variance in the image's texture is a strong indicator of authenticity. Skewness and curtosis, related to the asymmetry and tailedness of the image's pixel distribution, also play significant roles; their lower values are associated with genuine banknotes.\n",
    "\n",
    "From an image recognition standpoint, these correlations are reasonable. Genuine banknotes typically have consistent and high-quality printing, leading to lower variance in image texture. Similarly, the uniformity in genuine banknotes might result in less skewness and curtosis in their pixel distribution. Entropy, indicating randomness in the image, has a smaller but still negative coefficient, suggesting that higher disorder might be more common in forged notes. Overall, the model's reliance on these features aligns with key principles in image processing and pattern recognition, where texture uniformity and symmetry are crucial in differentiating authentic objects from forgeries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bcb91b",
   "metadata": {},
   "source": [
    "## Summary\n",
    "Congratulations for completing this exercise! In this notebook, with hands-on practice of linear models for regression and classification tasks, we gain deep understanding of:\n",
    "- overfitting VS underfitting\n",
    "- difference between l1 and l2 regularizations\n",
    "- the effect of regularization strength on model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327df471",
   "metadata": {},
   "source": [
    "## Which part(s) you find most interesting/chanlleging?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef02a79",
   "metadata": {},
   "source": [
    "Modifiying the parameters in a regression model can have a significant impact on the model's performance. For example, the penalty parameter c can be used to control the trade-off between the model's complexity and the model's ability to fit the training data. The following table shows the performance of logistic regression models with different c values and penalty parameters.\n",
    "Adjusting parameters like the regularization strength (C value) and the type of penalty (L1 or L2) can significantly alter the model's ability to generalize to new data. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
